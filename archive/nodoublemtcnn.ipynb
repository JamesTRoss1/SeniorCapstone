{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "pip install DeepFace\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqPyAd0sDsIZ",
        "outputId": "d10fd726-7893-4312-96ac-912d73c3ffe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting DeepFace\n",
            "  Downloading deepface-0.0.89-py3-none-any.whl (94 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/94.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m92.2/94.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.3/94.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from DeepFace) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from DeepFace) (1.5.3)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from DeepFace) (4.7.3)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from DeepFace) (4.66.2)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from DeepFace) (9.4.0)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.10/dist-packages (from DeepFace) (4.8.0.76)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from DeepFace) (2.15.0)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from DeepFace) (2.15.0)\n",
            "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from DeepFace) (2.2.5)\n",
            "Collecting mtcnn>=0.1.0 (from DeepFace)\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting retina-face>=0.0.1 (from DeepFace)\n",
            "  Downloading retina_face-0.0.16-py3-none-any.whl (25 kB)\n",
            "Collecting fire>=0.4.0 (from DeepFace)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gunicorn>=20.1.0 (from DeepFace)\n",
            "  Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire>=0.4.0->DeepFace) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.4.0->DeepFace) (2.4.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->DeepFace) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->DeepFace) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->DeepFace) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->DeepFace) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->DeepFace) (3.13.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->DeepFace) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->DeepFace) (4.12.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gunicorn>=20.1.0->DeepFace) (24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->DeepFace) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->DeepFace) (2023.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->DeepFace) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->DeepFace) (0.43.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=1.1.2->DeepFace) (2.1.5)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->DeepFace) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->DeepFace) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->DeepFace) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->DeepFace) (0.7.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=3.10.1->DeepFace) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->DeepFace) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->DeepFace) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->DeepFace) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->DeepFace) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->DeepFace) (1.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->DeepFace) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->DeepFace) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->DeepFace) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->DeepFace) (1.4.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->DeepFace) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->DeepFace) (3.2.2)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=9e80f21a4ec89dce479676084a4f02d361edc5bb351284ef54a601e132b9069f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built fire\n",
            "Installing collected packages: gunicorn, fire, mtcnn, retina-face, DeepFace\n",
            "Successfully installed DeepFace-0.0.89 fire-0.6.0 gunicorn-21.2.0 mtcnn-0.1.1 retina-face-0.0.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import islice\n",
        "import time\n"
      ],
      "metadata": {
        "id": "Rdm4rWO8euGc"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from deepface import DeepFace\n",
        "\n",
        "\n",
        "def emotion_faces():\n",
        "    \"\"\"\n",
        "    Determines the emotions of a collection of faces in a frame\n",
        "\n",
        "    :param list faces: a numpy array in BGR format, base64 encoded image, or path to the image\n",
        "    :return: string of the most prevalent emotion, dictionary of weighted emotion sums of all faces in an image\n",
        "    :rtype: str, dict\n",
        "    \"\"\"\n",
        "\n",
        "    #preds is a list of dictionaries; refer to https://github.com/serengil/deepface/blob/master/deepface/DeepFace.py for detailed documentation of analyze()\n",
        "    preds = DeepFace.stream(db_path = \"C:\\Users\\jacob\\Desktop\\Test\",  detector_backend=\"mtcnn\")\n",
        "    emotions = {}\n",
        "    weights = {'sad': 1, 'angry': 1, 'surprise': 1, 'fear': 1, 'happy': 1, 'disgust': 1, 'neutral': 1}\n",
        "\n",
        "    for pred in preds:\n",
        "        #print(\"Face Analysis: \" + str(pred))\n",
        "\n",
        "        #reliability of the model, don't process bad data. need to check if 0.8 is too high or too low\n",
        "        if pred[\"face_confidence\"] < 0.8:\n",
        "            pass\n",
        "        else:\n",
        "            emotions = {k: emotions.get(k, 0) + pred['emotion'].get(k, 0) for k in set(emotions) | set(pred['emotion'])}\n",
        "\n",
        "    #weight the sums\n",
        "    emotions = {k: emotions.get(k, 0) * weights.get(k, 0) for k in set(emotions) & set(weights)}\n",
        "\n",
        "    #Determine the most prevalent emotion collected\n",
        "    return max(emotions, key=emotions.get), emotions\n"
      ],
      "metadata": {
        "id": "_KR5Mn3MD2F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from deepface import DeepFace\n",
        "from mtcnn import MTCNN\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "x9I6eya2D8QI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V2bW97QnP0eP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def blur_measure(image):\n",
        "    \"\"\"\n",
        "    Calculates the variance of Laplacian to measure image blurriness\n",
        "\n",
        "    :param numpy.ndarray image: Input image\n",
        "    :return: Variance of Laplacian\n",
        "    :rtype: float\n",
        "    \"\"\"\n",
        "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
        "\n",
        "def emotion_faces(faces):\n",
        "    \"\"\"\n",
        "    Determines the emotions of a collection of faces in a frame\n",
        "\n",
        "    :param list faces: a numpy array in BGR format, base64 encoded image, or path to the image\n",
        "    :return: string of the most prevalent emotion, dictionary of weighted emotion sums of all faces in an image\n",
        "    :rtype: str, dict\n",
        "    \"\"\"\n",
        "\n",
        "    #preds is a list of dictionaries; refer to https://github.com/serengil/deepface/blob/master/deepface/DeepFace.py for detailed documentation of analyze()\n",
        "    preds = DeepFace.analyze(faces, actions=['emotion'], detector_backend=\"mtcnn\", enforce_detection=False)\n",
        "    emotions = {}\n",
        "    weights = {'sad': 1, 'angry': 1, 'surprise': 1, 'fear': 1, 'happy': 1, 'disgust': 1, 'neutral': 1}\n",
        "\n",
        "    for pred in preds:\n",
        "        print(\"Face Analysis: \" + str(pred))\n",
        "\n",
        "        # reliability of the model, don't process bad data. need to check if 0.8 is too high or too low\n",
        "        if pred[\"face_confidence\"] < 0.8:\n",
        "            pass\n",
        "        else:\n",
        "            emotions = {k: emotions.get(k, 0) + pred['emotion'].get(k, 0) for k in set(emotions) | set(pred['emotion'])}\n",
        "\n",
        "    # weight the sums\n",
        "    emotions = {k: emotions.get(k, 0) * weights.get(k, 0) for k in set(emotions) & set(weights)}\n",
        "\n",
        "    # Check if emotions dictionary is empty\n",
        "    if not emotions:\n",
        "        return \"Unknown\", {}\n",
        "\n",
        "    # Determine the most prevalent emotion collected\n",
        "    return max(emotions, key=emotions.get), emotions\n",
        "\n",
        "# Load the video\n",
        "#Link to site with sample videos: https://www.pexels.com/search/videos/group%20of%20people/\n",
        "video_path = \"video_3.mp4\"\n",
        "video_capture = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Set frame rate\n",
        "# Frame rate (frames per second)\n",
        "fps = 2\n",
        "frame_count = 0\n",
        "\n",
        "# Create MTCNN detector\n",
        "detector = MTCNN()\n",
        "\n",
        "# Iterate through frames\n",
        "timestart = time.time()\n",
        "while video_capture.isOpened():\n",
        "    ret, frame = video_capture.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Capture frame every 'fps' seconds\n",
        "    frame_count += 1\n",
        "    if frame_count % int(video_capture.get(cv2.CAP_PROP_FPS) * fps) == 0:\n",
        "\n",
        "        print(f\"Processing frame {frame_count}\")\n",
        "\n",
        "        # Detect faces\n",
        "        faces = detector.detect_faces(frame)\n",
        "        cropped_faces = []\n",
        "\n",
        "        # Extract faces and append to cropped_faces list\n",
        "        for face in faces:\n",
        "            x, y, w, h = face['box']\n",
        "            cropped_face = frame[y:y + h, x:x + w]\n",
        "\n",
        "            # Check blurriness of the face\n",
        "            #Can include this later as needed**\n",
        "            # blur_num = blur_measure(cropped_face)\n",
        "            # if blur_num < 100:  # Adjust threshold as needed, from testing threshold of 50 or less seems reasonable\n",
        "            #     print(f\"Face too blurry, skipping. measure: {blur_num}\")\n",
        "            #     continue  # Skip this face if too blurry\n",
        "\n",
        "            cropped_faces.append(cropped_face)\n",
        "\n",
        "        # Check if any faces were detected\n",
        "        if len(cropped_faces) == 0:\n",
        "            print(\"No faces detected in this frame.\")\n",
        "            continue  # Skip this frame if no faces are detected\n",
        "\n",
        "        # Output the list of cropped faces\n",
        "        for i, face in enumerate(cropped_faces):\n",
        "            emotion, emotions = emotion_faces(face)\n",
        "            if emotion:\n",
        "                cv2.imwrite(f\"face_{frame_count}_{i+1}_{emotion}.jpg\", face)\n",
        "                print(f\"Emotion of face {i+1} in frame {frame_count}: {emotion}, Emotion Details: {emotions}\")\n",
        "\n",
        "video_capture.release()\n",
        "cv2.destroyAllWindows()\n",
        "endtime = time.time()\n",
        "totaltime = endtime-timestart\n",
        "print(\"The total processing time was \" + str(totaltime))\n",
        "\n"
      ],
      "metadata": {
        "id": "NK2TcHU6kWg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c721a87d-4c4c-4db5-a3aa-0b1a03acb5b0"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing frame 50\n",
            "1/1 [==============================] - 1s 668ms/step\n",
            "1/1 [==============================] - 1s 515ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "24/24 [==============================] - 1s 13ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "3/3 [==============================] - 0s 8ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Face Analysis: {'emotion': {'angry': 3.5029876644556906e-07, 'disgust': 4.931812571655658e-15, 'fear': 1.2366279571362582e-08, 'happy': 99.99930262561456, 'sad': 0.00020173478171354022, 'surprise': 6.234161647682429e-06, 'neutral': 0.0004884964546491607}, 'dominant_emotion': 'happy', 'region': {'x': 0, 'y': 0, 'w': 231, 'h': 307, 'left_eye': (79, 106), 'right_eye': (195, 118)}, 'face_confidence': 1.0}\n",
            "Emotion of face 1 in frame 50: happy, Emotion Details: {'angry': 3.5029876644556906e-07, 'fear': 1.2366279571362582e-08, 'surprise': 6.234161647682429e-06, 'happy': 99.99930262561456, 'neutral': 0.0004884964546491607, 'sad': 0.00020173478171354022, 'disgust': 4.931812571655658e-15}\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Face Analysis: {'emotion': {'angry': 1.2482612565467213e-06, 'disgust': 3.884079958637906e-12, 'fear': 9.253224860164606e-08, 'happy': 99.96469021264431, 'sad': 0.00013185726422052963, 'surprise': 1.5042014550465927e-05, 'neutral': 0.03515935549475485}, 'dominant_emotion': 'happy', 'region': {'x': 0, 'y': 0, 'w': 195, 'h': 286, 'left_eye': (54, 113), 'right_eye': (152, 112)}, 'face_confidence': 1.0}\n",
            "Emotion of face 2 in frame 50: happy, Emotion Details: {'angry': 1.2482612565467213e-06, 'fear': 9.253224860164606e-08, 'surprise': 1.5042014550465927e-05, 'happy': 99.96469021264431, 'neutral': 0.03515935549475485, 'sad': 0.00013185726422052963, 'disgust': 3.884079958637906e-12}\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Face Analysis: {'emotion': {'angry': 2.9575208033276492e-05, 'disgust': 1.1019967364478395e-07, 'fear': 2.4173874674959825e-05, 'happy': 99.98612403704215, 'sad': 0.000728277559989507, 'surprise': 6.015181070878783e-07, 'neutral': 0.01309803057373307}, 'dominant_emotion': 'happy', 'region': {'x': 1, 'y': 0, 'w': 237, 'h': 324, 'left_eye': (50, 125), 'right_eye': (161, 133)}, 'face_confidence': 1.0}\n",
            "Emotion of face 3 in frame 50: happy, Emotion Details: {'angry': 2.9575208033276492e-05, 'fear': 2.4173874674959825e-05, 'surprise': 6.015181070878783e-07, 'happy': 99.98612403704215, 'neutral': 0.01309803057373307, 'sad': 0.000728277559989507, 'disgust': 1.1019967364478395e-07}\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "3/3 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Face Analysis: {'emotion': {'angry': 6.207266146397716e-11, 'disgust': 2.8221244075898424e-18, 'fear': 3.958744096421718e-09, 'happy': 99.57041144371033, 'sad': 1.546555772335978e-07, 'surprise': 2.6345410297778926e-07, 'neutral': 0.42959381826221943}, 'dominant_emotion': 'happy', 'region': {'x': 0, 'y': 0, 'w': 215, 'h': 279, 'left_eye': (60, 103), 'right_eye': (170, 118)}, 'face_confidence': 1.0}\n",
            "Emotion of face 4 in frame 50: happy, Emotion Details: {'angry': 6.207266146397716e-11, 'fear': 3.958744096421718e-09, 'surprise': 2.6345410297778926e-07, 'happy': 99.57041144371033, 'neutral': 0.42959381826221943, 'sad': 1.546555772335978e-07, 'disgust': 2.8221244075898424e-18}\n",
            "Processing frame 100\n",
            "1/1 [==============================] - 0s 277ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "22/22 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Face Analysis: {'emotion': {'angry': 5.334790579091006e-07, 'disgust': 5.1962052318930207e-14, 'fear': 0.00026863045623934443, 'happy': 95.50176805278875, 'sad': 0.0005318507826002549, 'surprise': 0.0002126181042224563, 'neutral': 4.497216060425766}, 'dominant_emotion': 'happy', 'region': {'x': 0, 'y': 0, 'w': 214, 'h': 257, 'left_eye': (55, 94), 'right_eye': (161, 94)}, 'face_confidence': 1.0}\n",
            "Emotion of face 1 in frame 100: happy, Emotion Details: {'angry': 5.334790579091006e-07, 'fear': 0.00026863045623934443, 'surprise': 0.0002126181042224563, 'happy': 95.50176805278875, 'neutral': 4.497216060425766, 'sad': 0.0005318507826002549, 'disgust': 5.1962052318930207e-14}\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Face Analysis: {'emotion': {'angry': 2.900671702832369e-06, 'disgust': 8.954597859652663e-12, 'fear': 8.498819425994952e-07, 'happy': 99.68761205673218, 'sad': 0.0001272957092623983, 'surprise': 0.0004799144335265737, 'neutral': 0.3117758082225919}, 'dominant_emotion': 'happy', 'region': {'x': 1, 'y': 5, 'w': 204, 'h': 276, 'left_eye': (61, 114), 'right_eye': (160, 113)}, 'face_confidence': 1.0}\n",
            "Emotion of face 2 in frame 100: happy, Emotion Details: {'angry': 2.900671702832369e-06, 'fear': 8.498819425994952e-07, 'surprise': 0.0004799144335265737, 'happy': 99.68761205673218, 'neutral': 0.3117758082225919, 'sad': 0.0001272957092623983, 'disgust': 8.954597859652663e-12}\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "3/3 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "Face Analysis: {'emotion': {'angry': 5.771948636675555e-10, 'disgust': 1.0534037965989219e-14, 'fear': 3.9872141630050033e-10, 'happy': 99.99641180081227, 'sad': 5.013006300883024e-06, 'surprise': 1.7478088289021376e-10, 'neutral': 0.0035797197877390934}, 'dominant_emotion': 'happy', 'region': {'x': 10, 'y': 0, 'w': 249, 'h': 291, 'left_eye': (80, 122), 'right_eye': (191, 97)}, 'face_confidence': 1.0}\n",
            "Emotion of face 3 in frame 100: happy, Emotion Details: {'angry': 5.771948636675555e-10, 'fear': 3.9872141630050033e-10, 'surprise': 1.7478088289021376e-10, 'happy': 99.99641180081227, 'neutral': 0.0035797197877390934, 'sad': 5.013006300883024e-06, 'disgust': 1.0534037965989219e-14}\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Face Analysis: {'emotion': {'angry': 2.2962715211957394e-06, 'disgust': 7.204146836260054e-09, 'fear': 6.054354073546577e-08, 'happy': 99.93551374250487, 'sad': 0.00014411601215254092, 'surprise': 4.249968758766131e-07, 'neutral': 0.06433727500478625}, 'dominant_emotion': 'happy', 'region': {'x': 1, 'y': 0, 'w': 227, 'h': 320, 'left_eye': (49, 115), 'right_eye': (152, 140)}, 'face_confidence': 1.0}\n",
            "Emotion of face 4 in frame 100: happy, Emotion Details: {'angry': 2.2962715211957394e-06, 'fear': 6.054354073546577e-08, 'surprise': 4.249968758766131e-07, 'happy': 99.93551374250487, 'neutral': 0.06433727500478625, 'sad': 0.00014411601215254092, 'disgust': 7.204146836260054e-09}\n",
            "Processing frame 150\n",
            "1/1 [==============================] - 0s 288ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "20/20 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Face Analysis: {'emotion': {'angry': 3.957220800935688e-06, 'disgust': 3.6239489040466042e-12, 'fear': 3.3316084532594535e-06, 'happy': 99.25780883524523, 'sad': 0.0001195303781670061, 'surprise': 0.003477189289480661, 'neutral': 0.7385899261939438}, 'dominant_emotion': 'happy', 'region': {'x': 0, 'y': 0, 'w': 189, 'h': 255, 'left_eye': (60, 91), 'right_eye': (148, 96)}, 'face_confidence': 1.0}\n",
            "Emotion of face 1 in frame 150: happy, Emotion Details: {'angry': 3.957220800935688e-06, 'fear': 3.3316084532594535e-06, 'surprise': 0.003477189289480661, 'happy': 99.25780883524523, 'neutral': 0.7385899261939438, 'sad': 0.0001195303781670061, 'disgust': 3.6239489040466042e-12}\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "3/3 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Face Analysis: {'emotion': {'angry': 2.4800567529691773e-05, 'disgust': 7.092789051552167e-11, 'fear': 0.00040579979832819737, 'happy': 18.806888901366126, 'sad': 2.0268471358004647, 'surprise': 1.0126209063162174e-05, 'neutral': 79.16581879069581}, 'dominant_emotion': 'neutral', 'region': {'x': 3, 'y': 0, 'w': 206, 'h': 260, 'left_eye': (63, 92), 'right_eye': (161, 95)}, 'face_confidence': 1.0}\n",
            "Emotion of face 2 in frame 150: neutral, Emotion Details: {'angry': 2.4800567529691773e-05, 'fear': 0.00040579979832819737, 'surprise': 1.0126209063162174e-05, 'happy': 18.806888901366126, 'neutral': 79.16581879069581, 'sad': 2.0268471358004647, 'disgust': 7.092789051552167e-11}\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Face Analysis: {'emotion': {'angry': 1.609705080340973e-06, 'disgust': 4.552491369991252e-09, 'fear': 3.63425600724554e-07, 'happy': 99.97729659080505, 'sad': 0.00026269458430761006, 'surprise': 3.06823477735918e-07, 'neutral': 0.02243997296318412}, 'dominant_emotion': 'happy', 'region': {'x': 3, 'y': 0, 'w': 200, 'h': 276, 'left_eye': (47, 106), 'right_eye': (139, 122)}, 'face_confidence': 1.0}\n",
            "Emotion of face 3 in frame 150: happy, Emotion Details: {'angry': 1.609705080340973e-06, 'fear': 3.63425600724554e-07, 'surprise': 3.06823477735918e-07, 'happy': 99.97729659080505, 'neutral': 0.02243997296318412, 'sad': 0.00026269458430761006, 'disgust': 4.552491369991252e-09}\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Face Analysis: {'emotion': {'angry': 2.644534906564881e-09, 'disgust': 1.3944464727358498e-13, 'fear': 3.6667912664197936e-10, 'happy': 99.9928474417743, 'sad': 7.915433945974435e-05, 'surprise': 5.4953953499565585e-08, 'neutral': 0.0070760165976641}, 'dominant_emotion': 'happy', 'region': {'x': 1, 'y': 17, 'w': 203, 'h': 223, 'left_eye': (76, 100), 'right_eye': (161, 95)}, 'face_confidence': 1.0}\n",
            "Emotion of face 4 in frame 150: happy, Emotion Details: {'angry': 2.644534906564881e-09, 'fear': 3.6667912664197936e-10, 'surprise': 5.4953953499565585e-08, 'happy': 99.9928474417743, 'neutral': 0.0070760165976641, 'sad': 7.915433945974435e-05, 'disgust': 1.3944464727358498e-13}\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Face Analysis: {'emotion': {'angry': 0.04607792943716049, 'disgust': 1.6901458366191946e-05, 'fear': 8.861705660820007, 'happy': 0.19814788829535246, 'sad': 90.89062213897705, 'surprise': 1.7880462666042263e-07, 'neutral': 0.0034330339985899627}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 70, 'h': 100, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}\n",
            "Emotion of face 5 in frame 150: Unknown, Emotion Details: {}\n",
            "Processing frame 200\n",
            "1/1 [==============================] - 0s 314ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "19/19 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Face Analysis: {'emotion': {'angry': 5.729215990513126e-08, 'disgust': 4.9378525986837673e-17, 'fear': 2.4273871107594047e-09, 'happy': 99.99744892120361, 'sad': 1.561044982167914e-06, 'surprise': 0.0001293268383051327, 'neutral': 0.0024178803869290277}, 'dominant_emotion': 'happy', 'region': {'x': 0, 'y': 0, 'w': 162, 'h': 239, 'left_eye': (56, 91), 'right_eye': (135, 101)}, 'face_confidence': 1.0}\n",
            "Emotion of face 1 in frame 200: happy, Emotion Details: {'angry': 5.729215990513126e-08, 'fear': 2.4273871107594047e-09, 'surprise': 0.0001293268383051327, 'happy': 99.99744892120361, 'neutral': 0.0024178803869290277, 'sad': 1.561044982167914e-06, 'disgust': 4.9378525986837673e-17}\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Face Analysis: {'emotion': {'angry': 2.1303249705860747e-06, 'disgust': 3.608310469793026e-08, 'fear': 4.142853168115881e-05, 'happy': 99.9604880762682, 'sad': 0.0011559529880406172, 'surprise': 7.653224034241594e-06, 'neutral': 0.038307659470830493}, 'dominant_emotion': 'happy', 'region': {'x': 1, 'y': 0, 'w': 185, 'h': 244, 'left_eye': (53, 98), 'right_eye': (137, 104)}, 'face_confidence': 1.0}\n",
            "Emotion of face 2 in frame 200: happy, Emotion Details: {'angry': 2.1303249705860747e-06, 'fear': 4.142853168115881e-05, 'surprise': 7.653224034241594e-06, 'happy': 99.9604880762682, 'neutral': 0.038307659470830493, 'sad': 0.0011559529880406172, 'disgust': 3.608310469793026e-08}\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "3/3 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Face Analysis: {'emotion': {'angry': 5.024405336229165e-08, 'disgust': 1.003408927428388e-14, 'fear': 3.7046810064111924e-07, 'happy': 95.10459899902344, 'sad': 0.038204676820896566, 'surprise': 2.126420289849662e-08, 'neutral': 4.857194423675537}, 'dominant_emotion': 'happy', 'region': {'x': 0, 'y': 0, 'w': 199, 'h': 264, 'left_eye': (66, 83), 'right_eye': (162, 108)}, 'face_confidence': 1.0}\n",
            "Emotion of face 3 in frame 200: happy, Emotion Details: {'angry': 5.024405336229165e-08, 'fear': 3.7046810064111924e-07, 'surprise': 2.126420289849662e-08, 'happy': 95.10459899902344, 'neutral': 4.857194423675537, 'sad': 0.038204676820896566, 'disgust': 1.003408927428388e-14}\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "3/3 [==============================] - 0s 8ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Face Analysis: {'emotion': {'angry': 8.21850043267445e-05, 'disgust': 2.6608544732615168e-08, 'fear': 0.0012781509974637663, 'happy': 98.66426579143202, 'sad': 0.013702747763112054, 'surprise': 0.00476589840430999, 'neutral': 1.3159060703758043}, 'dominant_emotion': 'happy', 'region': {'x': 8, 'y': 0, 'w': 211, 'h': 268, 'left_eye': (71, 103), 'right_eye': (171, 102)}, 'face_confidence': 1.0}\n",
            "Emotion of face 4 in frame 200: happy, Emotion Details: {'angry': 8.21850043267445e-05, 'fear': 0.0012781509974637663, 'surprise': 0.00476589840430999, 'happy': 98.66426579143202, 'neutral': 1.3159060703758043, 'sad': 0.013702747763112054, 'disgust': 2.6608544732615168e-08}\n",
            "The total processing time was 33.22556495666504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def blur_measure(image):\n",
        "    \"\"\"\n",
        "    Calculates the variance of Laplacian to measure image blurriness\n",
        "\n",
        "    :param numpy.ndarray image: Input image\n",
        "    :return: Variance of Laplacian\n",
        "    :rtype: float\n",
        "    \"\"\"\n",
        "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
        "\n",
        "def emotion_faces(faces):\n",
        "    \"\"\"\n",
        "    Determines the emotions of a collection of faces in a frame\n",
        "\n",
        "    :param list faces: a numpy array in BGR format, base64 encoded image, or path to the image\n",
        "    :return: string of the most prevalent emotion, dictionary of weighted emotion sums of all faces in an image\n",
        "    :rtype: str, dict\n",
        "    \"\"\"\n",
        "\n",
        "    #preds is a list of dictionaries; refer to https://github.com/serengil/deepface/blob/master/deepface/DeepFace.py for detailed documentation of analyze()\n",
        "    preds = DeepFace.analyze(faces, actions=['emotion'], detector_backend=\"mtcnn\", enforce_detection=False)\n",
        "    emotions = {}\n",
        "    weights = {'sad': 1, 'angry': 1, 'surprise': 1, 'fear': 1, 'happy': 1, 'disgust': 1, 'neutral': 1}\n",
        "\n",
        "    for pred in preds:\n",
        "        print(\"Face Analysis: \" + str(pred))\n",
        "\n",
        "        # reliability of the model, don't process bad data. need to check if 0.8 is too high or too low\n",
        "        if pred[\"face_confidence\"] < 0.8:\n",
        "            pass\n",
        "        else:\n",
        "            emotions = {k: emotions.get(k, 0) + pred['emotion'].get(k, 0) for k in set(emotions) | set(pred['emotion'])}\n",
        "\n",
        "    # weight the sums\n",
        "    emotions = {k: emotions.get(k, 0) * weights.get(k, 0) for k in set(emotions) & set(weights)}\n",
        "\n",
        "    # Check if emotions dictionary is empty\n",
        "    if not emotions:\n",
        "        return \"Unknown\", {}\n",
        "\n",
        "    # Determine the most prevalent emotion collected\n",
        "    return max(emotions, key=emotions.get), emotions\n",
        "\n",
        "# Load the video\n",
        "#Link to site with sample videos: https://www.pexels.com/search/videos/group%20of%20people/\n",
        "video_path = \"video_3.mp4\"\n",
        "video_capture = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Set frame rate\n",
        "# Frame rate (frames per second)\n",
        "fps = 2\n",
        "frame_count = 0\n",
        "\n",
        "# Create MTCNN detector\n",
        "detector = MTCNN()\n",
        "\n",
        "# Iterate through frames\n",
        "timestart = time.time()\n",
        "while video_capture.isOpened():\n",
        "    ret, frame = video_capture.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Capture frame every 'fps' seconds\n",
        "    frame_count += 1\n",
        "    if frame_count % int(video_capture.get(cv2.CAP_PROP_FPS) * fps) == 0:\n",
        "\n",
        "        print(emotion_faces(frame))\n",
        "video_capture.release()\n",
        "cv2.destroyAllWindows()\n",
        "endtime = time.time()\n",
        "totaltime = endtime-timestart\n",
        "print(\"The total processing time was \" + str(totaltime))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hW0V7THEA8u",
        "outputId": "25e0b23f-f6a1-44ea-d4ce-09f759a570b1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 341ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "32/32 [==============================] - 0s 9ms/step\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Face Analysis: {'emotion': {'angry': 8.699236331288773e-08, 'disgust': 9.69217517658369e-12, 'fear': 5.229933153216848e-08, 'happy': 99.78564381599426, 'sad': 8.835223184178176e-05, 'surprise': 1.1515107090076526e-06, 'neutral': 0.21426640450954437}, 'dominant_emotion': 'happy', 'region': {'x': 524, 'y': 311, 'w': 237, 'h': 333, 'left_eye': (607, 436), 'right_eye': (724, 450)}, 'face_confidence': 1.0}\n",
            "Face Analysis: {'emotion': {'angry': 1.7751172753932565e-08, 'disgust': 1.236636833302302e-19, 'fear': 1.160357210707244e-12, 'happy': 100.0, 'sad': 1.0675012873662126e-06, 'surprise': 7.027192716613229e-09, 'neutral': 2.3983302455121702e-06}, 'dominant_emotion': 'happy', 'region': {'x': 946, 'y': 177, 'w': 225, 'h': 298, 'left_eye': (1018, 300), 'right_eye': (1118, 299)}, 'face_confidence': 1.0}\n",
            "Face Analysis: {'emotion': {'angry': 1.5129628959397342e-15, 'disgust': 2.6183079921519256e-22, 'fear': 4.2373082396893234e-10, 'happy': 99.71210954857526, 'sad': 3.3837663856057515e-09, 'surprise': 3.759396351811278e-08, 'neutral': 0.28789215108854477}, 'dominant_emotion': 'happy', 'region': {'x': 231, 'y': 172, 'w': 223, 'h': 290, 'left_eye': (300, 283), 'right_eye': (408, 298)}, 'face_confidence': 1.0}\n",
            "Face Analysis: {'emotion': {'angry': 2.9817453395253324e-06, 'disgust': 4.418293231278625e-08, 'fear': 1.4298440653703892e-06, 'happy': 99.99911785125732, 'sad': 0.00013776179912383668, 'surprise': 2.6643212636123792e-08, 'neutral': 0.0007387439382000593}, 'dominant_emotion': 'happy', 'region': {'x': 1478, 'y': 277, 'w': 253, 'h': 337, 'left_eye': (1530, 413), 'right_eye': (1641, 420)}, 'face_confidence': 1.0}\n",
            "('happy', {'angry': 3.0864888771051157e-06, 'fear': 1.4825682880837374e-06, 'surprise': 1.2227750778785024e-06, 'happy': 399.49687121582684, 'neutral': 0.5028996978665348, 'sad': 0.00022718491601937026, 'disgust': 4.419262448808676e-08})\n",
            "1/1 [==============================] - 0s 279ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "30/30 [==============================] - 0s 13ms/step\n",
            "2/2 [==============================] - 0s 26ms/step\n",
            "Face Analysis: {'emotion': {'angry': 1.2350024240966034e-05, 'disgust': 2.751587041397542e-10, 'fear': 2.2567479770291623e-05, 'happy': 96.3350002366304, 'sad': 0.0003136976910200394, 'surprise': 0.005847795212974039, 'neutral': 3.6587952931171306}, 'dominant_emotion': 'happy', 'region': {'x': 1009, 'y': 225, 'w': 217, 'h': 272, 'left_eye': (1073, 333), 'right_eye': (1172, 331)}, 'face_confidence': 1.0}\n",
            "Face Analysis: {'emotion': {'angry': 1.4856891487457363e-11, 'disgust': 2.0474380725767135e-16, 'fear': 6.15246894229704e-11, 'happy': 99.99998807907104, 'sad': 3.791981395551147e-07, 'surprise': 1.454287299580085e-12, 'neutral': 9.261658107106996e-06}, 'dominant_emotion': 'happy', 'region': {'x': 543, 'y': 311, 'w': 248, 'h': 304, 'left_eye': (614, 441), 'right_eye': (728, 418)}, 'face_confidence': 1.0}\n",
            "Face Analysis: {'emotion': {'angry': 0.00013495278591241772, 'disgust': 8.558888687777794e-08, 'fear': 1.4953653198328213e-07, 'happy': 99.98940229542228, 'sad': 0.00021899608900252554, 'surprise': 8.336561002485009e-08, 'neutral': 0.010243401136337458}, 'dominant_emotion': 'happy', 'region': {'x': 1557, 'y': 297, 'w': 247, 'h': 333, 'left_eye': (1607, 425), 'right_eye': (1716, 446)}, 'face_confidence': 1.0}\n",
            "Face Analysis: {'emotion': {'angry': 1.242003722090601e-09, 'disgust': 9.350320604123867e-16, 'fear': 7.563782489228288e-07, 'happy': 99.3020236492157, 'sad': 4.898791416962922e-06, 'surprise': 4.669286068903489e-08, 'neutral': 0.6979700177907944}, 'dominant_emotion': 'happy', 'region': {'x': 252, 'y': 182, 'w': 216, 'h': 274, 'left_eye': (313, 290), 'right_eye': (416, 290)}, 'face_confidence': 1.0}\n",
            "('happy', {'angry': 0.00014730406701399734, 'fear': 2.3473456075887157e-05, 'surprise': 0.00584792527289904, 'happy': 395.6264142603394, 'neutral': 4.367017973702369, 'sad': 0.000537971769579083, 'disgust': 8.586404672169355e-08})\n",
            "1/1 [==============================] - 0s 258ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "30/30 [==============================] - 0s 9ms/step\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Face Analysis: {'emotion': {'angry': 8.625585616689242e-06, 'disgust': 1.414175411441561e-11, 'fear': 3.967699295749952e-06, 'happy': 99.96581077983251, 'sad': 0.0002072481473401884, 'surprise': 5.744807819625338e-05, 'neutral': 0.03390638044568086}, 'dominant_emotion': 'happy', 'region': {'x': 976, 'y': 281, 'w': 188, 'h': 248, 'left_eye': (1034, 379), 'right_eye': (1122, 384)}, 'face_confidence': 1.0}\n",
            "Face Analysis: {'emotion': {'angry': 3.7046491633933865e-11, 'disgust': 5.364107040138468e-16, 'fear': 2.1404736316732453e-08, 'happy': 99.84582662582397, 'sad': 4.870906309406564e-05, 'surprise': 5.54801170282726e-10, 'neutral': 0.15412861248478293}, 'dominant_emotion': 'happy', 'region': {'x': 268, 'y': 217, 'w': 215, 'h': 268, 'left_eye': (335, 322), 'right_eye': (435, 325)}, 'face_confidence': 1.0}\n",
            "Face Analysis: {'emotion': {'angry': 3.521833136232999e-06, 'disgust': 6.968545879226795e-09, 'fear': 2.397599097037073e-06, 'happy': 99.99144077301025, 'sad': 0.0004729519332613563, 'surprise': 9.414197199575369e-08, 'neutral': 0.00808272379799746}, 'dominant_emotion': 'happy', 'region': {'x': 1456, 'y': 392, 'w': 200, 'h': 269, 'left_eye': (1500, 494), 'right_eye': (1593, 512)}, 'face_confidence': 1.0}\n",
            "Face Analysis: {'emotion': {'angry': 5.684035937780253e-12, 'disgust': 1.0613647083896504e-15, 'fear': 4.7896783761380846e-09, 'happy': 99.99748468399048, 'sad': 1.347737121903947e-06, 'surprise': 1.5593484281595238e-07, 'neutral': 0.00251691435551038}, 'dominant_emotion': 'happy', 'region': {'x': 572, 'y': 342, 'w': 214, 'h': 231, 'left_eye': (648, 432), 'right_eye': (736, 421)}, 'face_confidence': 1.0}\n",
            "('happy', {'angry': 1.2147461483449813e-05, 'fear': 6.391492807479895e-06, 'surprise': 5.769870981223537e-05, 'happy': 399.8005628626572, 'neutral': 0.19863463108397164, 'sad': 0.0007302568808175143, 'disgust': 6.982689231116623e-09})\n",
            "1/1 [==============================] - 0s 293ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "28/28 [==============================] - 0s 9ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "Face Analysis: {'emotion': {'angry': 3.601187756885338e-05, 'disgust': 1.6106941084712e-10, 'fear': 1.5185023016783816e-06, 'happy': 99.80260729789734, 'sad': 0.001980673368962016, 'surprise': 8.101730486487213e-05, 'neutral': 0.1952991122379899}, 'dominant_emotion': 'happy', 'region': {'x': 994, 'y': 289, 'w': 172, 'h': 227, 'left_eye': (1053, 375), 'right_eye': (1133, 385)}, 'face_confidence': 1.0}\n",
            "Face Analysis: {'emotion': {'angry': 1.6046671547087499e-06, 'disgust': 1.361041169361827e-12, 'fear': 2.693578835533117e-05, 'happy': 84.9250078201294, 'sad': 0.020080182002857327, 'surprise': 9.978895931794796e-07, 'neutral': 15.054881572723389}, 'dominant_emotion': 'happy', 'region': {'x': 357, 'y': 189, 'w': 211, 'h': 275, 'left_eye': (431, 287), 'right_eye': (524, 314)}, 'face_confidence': 1.0}\n",
            "Face Analysis: {'emotion': {'angry': 1.721148095157332e-05, 'disgust': 7.045123373572437e-08, 'fear': 0.0002452473381708842, 'happy': 95.115727186203, 'sad': 0.010033812577603385, 'surprise': 0.0334517186274752, 'neutral': 4.840530455112457}, 'dominant_emotion': 'happy', 'region': {'x': 1419, 'y': 416, 'w': 188, 'h': 242, 'left_eye': (1475, 514), 'right_eye': (1561, 521)}, 'face_confidence': 1.0}\n",
            "Face Analysis: {'emotion': {'angry': 7.013012037759836e-06, 'disgust': 5.770322414333506e-09, 'fear': 3.338651595186093e-05, 'happy': 99.83213543891907, 'sad': 0.00304684854199877, 'surprise': 0.001542300196888391, 'neutral': 0.16323839081451297}, 'dominant_emotion': 'happy', 'region': {'x': 683, 'y': 385, 'w': 196, 'h': 276, 'left_eye': (751, 488), 'right_eye': (849, 490)}, 'face_confidence': 1.0}\n",
            "('happy', {'angry': 6.184103771289529e-05, 'fear': 0.00030708814477975466, 'surprise': 0.035076034018821645, 'happy': 379.6754777431488, 'neutral': 20.25394953088835, 'sad': 0.0351415164914215, 'disgust': 7.638398660207436e-08})\n",
            "The total processing time was 17.32966899871826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RyYdJ6JJlPHA"
      }
    }
  ]
}